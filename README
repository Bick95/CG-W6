Readme Open GL Assignment 2 (see below for Readme of 'OpenGL Assignment 1')


For this assignment, we built up on our code submitted for the Open GL Assignment 1, since the code worked without any problems. 

In the first step of this assignment, we implemented the three illumination models Normal-, Gouraud-, and Phong shading in the respective shader-files contained in the code. 

In addition to the three variables passed to all 3 vertex shaders, which are modelTransform (transformation matrix for transformation of the model displayed), projectionTransform (responsible for transforming the projection), preserveNormals (to preserve the original relative angles of the normals of the model displayed after the model transformation), we also included the uniform-variables lightPos, lightColor, materialColor, and coeffs as inputs to both the vertex shaders of the Gouraud and Phong illumination models, giving the position of the (unique) light source, the color of the light source, the material color, and the 3 illumination coefficients (= ka, kd, and ks; combined in a struct) respectively. 

While for the Gouraud model, most computations to be implemented happen in the vertex shader, most of the computations to be implemnted for the Phong model happen in the respective (Phong) fragment shader, since in this model the color values are computed per pixel instead of receiving them already interpolated from the vertex shader, as is the case in the Gouraud model.  Therefore the Gouraud fragment shader basically takes the calculated light value from the vertex and outputs it (with the addition of the texture, but that comes later).

In the Normal illumination model, the interpolated normals per pixel arrive in the Normal fragment shader, after the adjusted normals per vertex get determined in the Normal vertex shader. Normals get 'adjusted' by applying the normal matrix to them. Finally, a mapping of the interpolated normals per fragment happens in the Normal fragment shader, mapping values from [-1,1] to [0,1], so to make them interpretable as color values.

The Phong shader is very similar in calculation to the Gouraud, but besides the normal calculation, everything happens in the fragment shader. The normal and the variables 
lightPos, lightColor, materialColor, and coeffs are simply passed further to the fragment shader, alongside with the relative position, as in the fragment we can only access position in terms of pixels.

Notes for the shaders: the shaders only work with 1 light source. The eye position is assumed to be at [0,0,0], and hard coded to that. The p exponent for the specular reflection is selected to be 3, also hard coded in the corresponding shaders. For Phong, the light position is automatically normalized when passing from the vertex to the fragment, so we multiply it by 10 to obtain the same position as in Gouraud. Also here, the uniforms height and width are included, but not used in the final model. The shaders produce the outputs with the textures, but the original color-outputs are still there as comments.

Selecting shaders happens by receiving action events whenever the user clicks on a respective Radio button for chosing another shader, which results in our implementation in a shading-mode index to be assigned to a dedicated variable which always stores the current shading mode index. Consequentially, the chosen shader (indicated by the variable) gets assigned to a shader pointer in a switch-statement, and the shader-object the pointer points to is then used for the shading the resulting image of the model to be displayed in the scene after the switch of the shading mode. 

As an extra, we implemented the possibility to change the position and the color of the light-source. One can change the position in the scene within the bounds [-10,10], where 0 is the center of the screen in all three directions: x, y, and z respectively through the manipulation of slider values. -/+ signs next to the sliders indicate in which direction the user is moving the light-source.  For the color, the values red, green and blue of the light source can be change by the corresponding sliders in the range of [0,1].

Speaking about extras, also the unitization-function has been updated to be universally applicable to all kinds of meshes/shapes now. Now, the function finds the max value from the set of all x-, y-, and z-coordinates in an object and divides afterwards all coordinate values belonging to the object by the previously determined maximal coordinate value. 

For implementing the texture mapping, we exactly followed the instructions from the slides. So there is not much to be reported about that part. One point to be mentioned, though, is how we decided to include the additional 2D coordinates for the texture mapping into our previous implementation. We included them as additional x- and y-coordinates in the vertex structure. Consequentially, the formatting of how to interpret the data stored in the VBO had to be adjusted to start a new vertex every 8 floats now. Also, we introduced a third location in all vertex shaders, being called mesh_coords, which is used for retrieving the appropriate 2D coordinates of where to look up the texture value for a given 3D position on a mesh.  

In both, the Gouraud and the Phong fragment shaders, we compute the final shading of a fragment/pixel by multiplying the corresponding texture value retrieved from the texture map for a given pixel by the computed illumination value at the same pixel.

This concludes the descriptions of new implementations and changes made to the code. The project has been tested on the university computers on Linux.


 

##########################
Readme OpenGL Assignment 1

For this assignment, we enabled passing transformation-matrices from the MainView-object, where they get updated, to the shader-object, where the transformations get applies to the vertex-positions.
Also, vertices had to be created for representing both the cube and the pyramid shapes. For simplicity, the same vertices are shared between both shapes (only in different combinations and being translated to different positions).

The global variables scalingFactor and rotationX, rotationY, rotationZ have been introduced to to store the user inputs commanded via the Q-Widgets globally. When repainting the shapes, the updated values of these variables are retrieved to generate an updated transformation matrix for each shape respectively. 

Also, a Mesh can be read in by now. "Unitization" (Normalization) has been implemented in the model by applying the QVector3D::normalize() function to each vertex stored in the model. A pseudo-random generator was included to the MainView to generate the random numbers specifying the Sphere's colors.  

Also, since multiple buffers had to be used for multiple shapes, the VBO and VAB have been changed to being arrays, where index = 0 refers to cube, index = 1 refers to pyramid, and index = 2 refers to sphere. These global arrays get freed in the MainView's destructor again.

For further information, please see the comments in the code.

The largest problem encountered was understanding the various aspects of how the VertexShader and the MainView communicate. We however, by now, understand how this mechanism works. 
Also, understanding the working of the QMatrix4x4 object was not easy in the beginning. Furthermore, initially not putting vertices in the "right" (counter-clockwise) order when defining shapes caused problems.

The project can be run by building and running it in the Qt environment.
